{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the fvt repo to the default path\n",
    "sys.path.append(os.path.abspath(\"../fast-vocabulary-transfer\"))\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_seq_length = 512\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "fourbit_models = [\n",
    "    'unsloth/gemma-2-9b-bnb-4bit',\n",
    "    'unsloth/gemma-2-27b-bnb-4bit'\n",
    "]\n",
    "\n",
    "model,tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name='unsloth/gemma-2-9b-bnb-4bit',max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.layers[0].self_attn.q_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset, disable_progress_bar\n",
    "\n",
    "# Disable hf_transfer in multiple ways\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = \"0\"\n",
    "os.environ['USE_TORCH'] = \"1\"  # Force using torch for downloads\n",
    "\n",
    "# Optional: Disable progress bars if they're causing issues\n",
    "disable_progress_bar()\n",
    "\n",
    "# Set longer timeout and chunk size\n",
    "os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = \"500\"  # 500 seconds timeout\n",
    "os.environ['HF_HUB_DOWNLOAD_CHUNK_SIZE'] = \"10485760\"  # 10MB chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading the sentiment dataset for evaluation\n",
    "sentiment_dataset = load_dataset(\n",
    "    'Prazzwal07/sentiment_analysis_nepglue'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_dataset['test'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMMA_PROMPT=\"\"\"\n",
    "<start_of_turn>user\n",
    "{}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "\n",
    "# def eval_iterator(hf_dataset):\n",
    "#     input_val =   GEMMA_PROMPT.format(hf_dataset[])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Specify cache directory\n",
    "cache_dir = \"/root/gemma2_finetuning/cache\"  # Replace with your path\n",
    "\n",
    "# Load dataset with more conservative settings\n",
    "iriis_train = load_dataset(\n",
    "    'IRIISNEPAL/Nepali-Text-Corpus', \n",
    "    split='train',\n",
    "    revision='main',\n",
    "    streaming=True,\n",
    "    download_mode='force_redownload',\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_nepali(values_dict):\n",
    "    \n",
    "    text = values_dict['Article']\n",
    "    # Handle empty or non-string input\n",
    "    if not text or not isinstance(text, str):\n",
    "        return \"\"\n",
    "        \n",
    "    # This step adds spaces before common Nepali suffixes (postpositions) that attach to words\n",
    "    # ले = ergative case marker, को = possessive, मा = locative, बाट = ablative, देखि = from, सम्म = until\n",
    "    # Example: \"घरमा\" -> \"घर मा\" (meaning \"in the house\")\n",
    "    text = re.sub(r'(ले|को|मा|बाट|देखि|सम्म)$', r' \\1', text)\n",
    "\n",
    "    # First remove non-Devanagari characters except word separators, whitespace and numbers\n",
    "    # text = re.sub(r'[^\\u0900-\\u097F\\s।0-9]', '', text)\n",
    "    \n",
    "    # Then add spaces after word separators | and । in a single step\n",
    "    text = re.sub(r'([।])(\\S)', r'\\1 \\2', text)\n",
    "    \n",
    "    # Remove extra spaces including newlines and tabs\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = preprocess_nepali({'Article': GEMMA_PROMPT.format('Classify this sentiment as positive(+1) negative(-1) or neutral(0) -- बिहीबार दिउँसो खोला तर्ने क्रममा एक जना बगेर बेपत्ता भएको प्रहरीले जनाएको छ।')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CUDA debugging flags\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "\n",
    "# Prepare inputs with position IDs\n",
    "\n",
    "max_length = 512  # Match your model's configuration\n",
    "inputs = tokenizer(\n",
    "    dummy_input, \n",
    "    return_tensors='pt',\n",
    "    max_length=max_length,\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    padding_side='right',\n",
    ").to('cuda')\n",
    "# \n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # %%\n",
    "# Add position IDs explicitly\n",
    "position_ids = torch.arange(\n",
    "        0, \n",
    "        inputs['input_ids'].shape[1], \n",
    "        dtype=torch.long, \n",
    "        device='cuda'\n",
    "    )\n",
    "inputs['position_ids'] = position_ids\n",
    "\n",
    "print(f\"position_ids: {inputs['position_ids']}\")\n",
    "# Update generation config\n",
    "generation_config = model.generation_config\n",
    "generation_config.use_cache = False\n",
    "generation_config.max_length = max_length\n",
    "\n",
    "# Generate model = model.to('cuda')\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    generation_config=generation_config,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "# Decode\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iriis_tokenizer_dataset = iriis_train.take(100000)\n",
    "#loading the tokenizer dataset\n",
    "mapped_tokenizer_iterator = map(preprocess_nepali, iriis_tokenizer_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Nepali-specific normalizers\n",
    "\n",
    "#preparing the tokenizer for training\n",
    "new_tokenizer = tokenizer.train_new_from_iterator(\n",
    "    mapped_tokenizer_iterator, \n",
    "    vocab_size=6400, \n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer.tokenize('बिहीबार दिउँसो खोला तर्ने क्रममा एक जना बगेर बेपत्ता भएको प्रहरीले जनाएको छ। शिवतासक्षी नगरपालिका–१० सुकुम्वासी बस्तीका ४३ वर्षीय साहेब सरदार माइ खोला तर्ने क्रममा दिउँसो १ बजेदेखि बेपत्ता भएका हुन्। खोला तर्ने क्रममा सरदार बगेको देखेपछि स्थानीयले प्रहरीलाई खबर गरेका थिए । इलाका प्रहरी कार्यालय झिलभिले, प्रहरी चौकी माइधार र स्थानीयवासीले बेपत्ता सरदारको खाजीकार्य गरिरहेको जिल्ला प्रहरी कार्यालय झापाका प्रवक्ता महेन्द्रकुमार श्रेष्ठले जानकारी दिए ।')[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fvt.fvt import FastVocabularyTransfer\n",
    "\n",
    "# let's create a new model withe new mapped embedding table\n",
    "fvt = FastVocabularyTransfer()\n",
    "new_model = fvt.transfer(\n",
    "    in_tokenizer=new_tokenizer,\n",
    "    gen_tokenizer=tokenizer,\n",
    "    gen_model=model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer,  TrainingArguments, DataCollatorForLanguageModeling\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "n_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0\n",
    "print(f\"Using {n_gpus} GPUs\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "def custom_prepare_model_for_kbit_training(model):\n",
    "    # 1. Enable input gradients - necessary for training\n",
    "    if hasattr(model, \"enable_input_require_grads\"):\n",
    "        model.enable_input_require_grads()\n",
    "    \n",
    "    # 2. Disable gradient checkpointing to avoid memory issues with Gemma\n",
    "    if hasattr(model, \"gradient_checkpointing_enable\"):\n",
    "        model.gradient_checkpointing_disable()\n",
    "    \n",
    "    # 3. Disable KV caching which can interfere with training\n",
    "    model.config.use_cache = False\n",
    "    \n",
    "    # 4. Convert 1D parameters (like bias terms and LayerNorm) to float32\n",
    "    # This is crucial because these parameters need higher precision\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.ndim == 1:  # bias or LayerNorm parameters\n",
    "            param.data = param.data.to(torch.float16)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = new_model\n",
    "model = model.to(device)\n",
    "\n",
    "# Prepare model for LoRA training\n",
    "model = custom_prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=2,\n",
    "    lora_alpha=4,\n",
    "    target_modules=['q_proj','v_proj'],\n",
    "    # target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dataset:\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "class NepaliIterableDataset(IterableDataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length=2048):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        # Set a fixed number of steps for the scheduler\n",
    "        self.num_examples = 1000000  # Adjust this based on how many examples you want to train on\n",
    "\n",
    "    def __iter__(self):\n",
    "        for item in self.dataset:\n",
    "            text = preprocess_nepali(item)\n",
    "            if text:\n",
    "                encodings = self.tokenizer(\n",
    "                    text, \n",
    "                    truncation=True,\n",
    "                    padding='max_length',\n",
    "                    max_length=self.max_length,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                yield {\n",
    "                    \"input_ids\": encodings[\"input_ids\"][0],\n",
    "                    \"attention_mask\": encodings[\"attention_mask\"][0],\n",
    "                    \"labels\": encodings[\"input_ids\"][0].clone()\n",
    "                }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_examples\n",
    "\n",
    "# Create data collator with padding\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=new_tokenizer, \n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=8  # Helps with GPU efficiency\n",
    ")\n",
    "\n",
    "\n",
    "train_dataset = NepaliIterableDataset(iriis_train, new_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CC'] = 'gcc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Update training arguments with device settings\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./nepali_lora_weights_ver2',\n",
    "    num_train_epochs=3,\n",
    "    warmup_steps=10,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    save_steps=500,\n",
    "    logging_steps=5,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_total_limit=2,\n",
    "    no_cuda=False,  # Enable CUDA\n",
    ")\n",
    "\n",
    "\n",
    "# Create data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=new_tokenizer, \n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # Your dataset\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the LoRA weights\n",
    "model.save_pretrained(\"./nepali_lora_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key='',  # This is the default and can be omitted\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Use this nepali idiom in a good sentence -- कांही नभएको जात्रा हाँडी गाउँमा. It should be structured such that user 1 says something and user 2 says this idiom in the response sentence\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-4o\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion2 = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Use this nepali idiom in a good sentence -- कसैलाई के धन्दा घरज्वाईंलाई खानको धन्दा. It should be structured such that user 1 says something and user 2 says this idiom in the response sentence\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-4o\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat_completion2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion3 = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Use this nepali idiom in a good sentence -- कच्चा वैद्यको मात्रा यमपुरीको यात्रा It should be structured such that user 1 says something and user 2 says this idiom in the response sentence\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-4o\",\n",
    ")\n",
    "\n",
    "print(chat_completion3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion3 = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Use this nepali idiom in a good sentence -- औंला दिंदा डुँडुल्ना निल्ने\tTry really hard to understand the context of the usage from previous examples in the internet. It should be structured such that user 1 says something and user 2 says this idiom in the response sentence\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-4o\",\n",
    ")\n",
    "\n",
    "print(chat_completion3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion4 = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Use this nepali idiom in a good sentence -- ओरालो लागेको मृगलाई बाच्छाले पनि खेद्छ\t\tTry really hard to understand the context of the usage from previous examples in the internet. It should be structured such that user 1 says something and user 2 says this idiom in the response sentence\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-4o\",\n",
    ")\n",
    "\n",
    "print(chat_completion4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion5 = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Use this nepali idiom in a good sentence -- एक हातले तालि बज्दैन\t\tTry really hard to understand the context of the usage from previous examples in the internet. It should be structured such that user 1 says something and user 2 says this idiom in the response sentence\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-4o\",\n",
    ")\n",
    "\n",
    "print(chat_completion5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion6 = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Use this nepali idiom in a good sentence -- एक कान दुई कान मैदान\t\tTry really hard to understand the context of the usage from previous examples in the internet. It should be structured such that user 1 says something and user 2 says this idiom in the response sentence\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-4o\",\n",
    ")\n",
    "\n",
    "print(chat_completion6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
